{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests  # For making HTTP requests to the Ollama API\n",
    "import threading\n",
    "import time  # For generating the `created` timestamp\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the Ollama server URL\n",
    "OLLAMA_API_URL = \"http://localhost:11434/v1/completions\"  # Update if using a different port\n",
    "\n",
    "@app.route('/v1/chat/completions', methods=['POST'])\n",
    "def completions():\n",
    "    # Parse the incoming JSON request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Extract the 'messages' list from the request\n",
    "    messages = data.get('messages', [])\n",
    "\n",
    "    # Check if there are messages\n",
    "    if not messages or not isinstance(messages, list):\n",
    "        return jsonify({\"error\": \"Invalid or missing 'messages' field\"}), 400\n",
    "\n",
    "    # Concatenate all user messages\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content')\n",
    "        if role == 'user' and content:\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "\n",
    "    # Default model parameters\n",
    "    max_tokens = data.get(\"max_tokens\", 100)\n",
    "    temperature = data.get(\"temperature\", 0.7)\n",
    "    top_p = data.get(\"top_p\", 1)\n",
    "    model = data.get(\"model\", \"llama3.1:8b\")  # Default model\n",
    "\n",
    "    # Error handling for empty prompt\n",
    "    if not prompt.strip():\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Send a request to the Ollama API\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens if max_tokens > 0 else 100,  # Ensure a valid max_tokens value\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            return jsonify({\"error\": f\"Failed to communicate with Ollama server: {response.text}\"}), response.status_code\n",
    "\n",
    "        # Extract the completion text from the Ollama server response\n",
    "        completion_response = response.json()\n",
    "        completion_text = completion_response['choices'][0]['text']\n",
    "\n",
    "        # Create the response object in the desired format\n",
    "        response = {\n",
    "            \"id\": \"chatcmpl-\" + str(int(time.time())),  # Unique ID for the completion\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),  # Unix timestamp\n",
    "            \"model\": model,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"message\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": completion_text  # Assistant's response\n",
    "                    },\n",
    "                    \"finish_reason\": completion_response['choices'][0].get('finish_reason', 'stop')\n",
    "                }\n",
    "            ],\n",
    "            # Assuming the API provides prompt and completion token usage\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": completion_response.get('prompt_tokens', 0),\n",
    "                \"completion_tokens\": completion_response.get('completion_tokens', len(completion_text.split())),\n",
    "                \"total_tokens\": completion_response.get('total_tokens', len(prompt.split()) + len(completion_text.split()))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return jsonify(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors and return a 500 status code with error message\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Function to run the Flask app in a separate thread\n",
    "def run_app():\n",
    "    app.run(debug=True, use_reloader=False, host='0.0.0.0', port=1234)\n",
    "\n",
    "# Start the Flask server in a background thread\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
