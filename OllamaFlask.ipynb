{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a model to your Ollama local setup and call it using Python, follow these general steps:\n",
    "\n",
    "1. Install Ollama\n",
    "Ensure you have the Ollama CLI installed on your system. If you haven't done this yet, download and install it from the [Ollama website](https://ollama.com/).\n",
    "\n",
    "2. Add a Model to Ollama\n",
    "Ollama has a built-in command to download and add models. To add a model, open your terminal and use the following command:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "3. Ensure Ollama is Running\n",
    "Make sure your Ollama instance is up and running locally. You can use:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "# Instructions for Running Flask API in Jupyter Notebook\n",
    "\n",
    "This guide will walk you through setting up a local Flask API that mimics OpenAI's `/v1/completions` endpoint using the Ollama model or another local language model.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure the following are installed:\n",
    "- **Flask**: Python web framework.\n",
    "- **Ollama** (or your preferred local LLM library): This guide assumes you're using Ollama.\n",
    "\n",
    "You can install them by running the following commands in a cell:\n",
    "\n",
    "```bash\n",
    "!pip install flask ollama\n",
    "```\n",
    "Steps to Set Up and Run the Flask App\n",
    "Write the Flask App:\n",
    "\n",
    "Copy the Flask app code provided below into a code cell and execute it.\n",
    "Run the Flask Server:\n",
    "\n",
    "You can run the Flask server directly from within a Jupyter Notebook. The server will be accessible at http://localhost:5000.\n",
    "Send Requests to the API:\n",
    "\n",
    "After the server is running, you can make POST requests to http://localhost:5000/v1/completions using tools like curl, Postman, or Python code within the notebook.\n",
    "Example Request:\n",
    "\n",
    "To test the API, you can use the following code inside the notebook:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:5000/v1/completions\"\n",
    "data = {\n",
    "    \"prompt\": \"What is the capital of France?\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"model\": \"llama3.1:8b\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n",
    "```\n",
    "Or you can test it with a tool like curl or Postman:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:5000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"prompt\": \"What is the capital of France?\",\n",
    "      \"max_tokens\": 50,\n",
    "      \"temperature\": 0.7,\n",
    "      \"model\": \"llama3.1:8b\"\n",
    "    }'\n",
    "```\n",
    "Stop the Server:\n",
    "When you're done, stop the Flask server by interrupting the kernel in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://192.168.1.128:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [11/Sep/2024 18:30:18] \"POST /v1/completions HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests  # For making HTTP requests to the Ollama API\n",
    "import threading\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the Ollama server URL\n",
    "OLLAMA_API_URL = \"http://localhost:11434/v1/completions\"  # Update if using a different port\n",
    "\n",
    "@app.route('/v1/completions', methods=['POST'])\n",
    "def completions():\n",
    "    # Parse the incoming JSON request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Extract necessary information from the request (mimicking OpenAI API)\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    max_tokens = data.get(\"max_tokens\", 100)\n",
    "    temperature = data.get(\"temperature\", 0.7)\n",
    "    top_p = data.get(\"top_p\", 1)\n",
    "    model = data.get(\"model\", \"llama3.1:8b\")  # Default to a model you have configured\n",
    "\n",
    "    # Error handling\n",
    "    if not prompt:\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Send a request to the Ollama API\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            return jsonify({\"error\": f\"Failed to communicate with Ollama server: {response.text}\"}), response.status_code\n",
    "\n",
    "        # Extract the completion text from the Ollama server response\n",
    "        completion_response = response.json()\n",
    "        completion_text = completion_response['choices'][0]['text']\n",
    "\n",
    "        # Format the response like the OpenAI API\n",
    "        response = {\n",
    "            \"id\": model,\n",
    "            \"object\": \"text_completion\",\n",
    "            \"created\": 1234567890,  # You can use actual timestamp if desired\n",
    "            \"model\": model,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"text\": completion_text,\n",
    "                    \"index\": 0,\n",
    "                    \"logprobs\": None,\n",
    "                    \"finish_reason\": \"length\" if len(completion_text.split()) >= max_tokens else \"stop\"\n",
    "                }\n",
    "            ],\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": len(prompt.split()),\n",
    "                \"completion_tokens\": len(completion_text.split()),\n",
    "                \"total_tokens\": len(prompt.split()) + len(completion_text.split())\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return jsonify(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors and return a 500 status code with error message\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Function to run the Flask app in a separate thread\n",
    "def run_app():\n",
    "    app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start the Flask server in a background thread\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
