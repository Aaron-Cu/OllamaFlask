{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install flask ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a model to your Ollama local setup and call it using Python, follow these general steps:\n",
    "\n",
    "1. Install Ollama\n",
    "Ensure you have the Ollama CLI installed on your system. If you haven't done this yet, download and install it from the [Ollama website](https://ollama.com/).\n",
    "\n",
    "2. Add a Model to Ollama\n",
    "Ollama has a built-in command to download and add models. To add a model, open your terminal and use the following command:\n",
    "\n",
    "```bash\n",
    "ollama pull llama3.1:8b\n",
    "```\n",
    "\n",
    "3. Ensure Ollama is Running\n",
    "Make sure your Ollama instance is up and running locally. You can use:\n",
    "\n",
    "```bash\n",
    "ollama serve\n",
    "```\n",
    "\n",
    "# Instructions for Running Flask API in Jupyter Notebook\n",
    "\n",
    "This guide will walk you through setting up a local Flask API that mimics OpenAI's `/v1/completions` endpoint using the Ollama model or another local language model.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure the following are installed:\n",
    "- **Flask**: Python web framework.\n",
    "- **Ollama** (or your preferred local LLM library): This guide assumes you're using Ollama.\n",
    "\n",
    "You can install them by running the following commands in a cell:\n",
    "\n",
    "```bash\n",
    "!pip install flask ollama\n",
    "```\n",
    "Steps to Set Up and Run the Flask App\n",
    "Write the Flask App:\n",
    "\n",
    "Copy the Flask app code provided below into a code cell and execute it.\n",
    "Run the Flask Server:\n",
    "\n",
    "You can run the Flask server directly from within a Jupyter Notebook. The server will be accessible at http://localhost:5000.\n",
    "Send Requests to the API:\n",
    "\n",
    "After the server is running, you can make POST requests to http://localhost:5000/v1/completions using tools like curl, Postman, or Python code within the notebook.\n",
    "Example Request:\n",
    "\n",
    "To test the API, you can use the following code inside the notebook:\n",
    "```python\n",
    "import requests\n",
    "\n",
    "url = \"http://localhost:5000/v1/completions\"\n",
    "data = {\n",
    "    \"prompt\": \"What is the capital of France?\",\n",
    "    \"max_tokens\": 50,\n",
    "    \"temperature\": 0.7,\n",
    "    \"model\": \"llama3.1:8b\"\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=data)\n",
    "print(response.json())\n",
    "```\n",
    "Or you can test it with a tool like curl or Postman:\n",
    "\n",
    "```bash\n",
    "curl -X POST http://localhost:5000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "      \"prompt\": \"What is the capital of France?\",\n",
    "      \"max_tokens\": 50,\n",
    "      \"temperature\": 0.7,\n",
    "      \"model\": \"llama3.1:8b\"\n",
    "    }'\n",
    "```\n",
    "Stop the Server:\n",
    "When you're done, stop the Flask server by interrupting the kernel in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests  # For making HTTP requests to the Ollama API\n",
    "import threading\n",
    "import time  # For generating the `created` timestamp\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Define the Ollama server URL\n",
    "OLLAMA_API_URL = \"http://localhost:11434/v1/completions\"  # Update if using a different port\n",
    "\n",
    "@app.route('/chat/completions', methods=['POST'])\n",
    "def completions():\n",
    "    # Parse the incoming JSON request\n",
    "    data = request.get_json()\n",
    "\n",
    "    # Extract the 'messages' list from the request\n",
    "    messages = data.get('messages', [])\n",
    "\n",
    "    # Check if there are messages\n",
    "    if not messages or not isinstance(messages, list):\n",
    "        return jsonify({\"error\": \"Invalid or missing 'messages' field\"}), 400\n",
    "\n",
    "    # Concatenate all user messages\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        role = message.get('role')\n",
    "        content = message.get('content')\n",
    "        if role == 'user' and content:\n",
    "            prompt += f\"User: {content}\\n\"\n",
    "\n",
    "    # Default model parameters\n",
    "    max_tokens = data.get(\"max_tokens\", 100)\n",
    "    temperature = data.get(\"temperature\", 0.7)\n",
    "    top_p = data.get(\"top_p\", 1)\n",
    "    model = data.get(\"model\", \"llama3.1:8b\")  # Default model\n",
    "\n",
    "    # Error handling for empty prompt\n",
    "    if not prompt.strip():\n",
    "        return jsonify({\"error\": \"Prompt is required\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Send a request to the Ollama API\n",
    "        response = requests.post(\n",
    "            OLLAMA_API_URL,\n",
    "            json={\n",
    "                \"model\": model,\n",
    "                \"prompt\": prompt,\n",
    "                \"max_tokens\": max_tokens if max_tokens > 0 else 100,  # Ensure a valid max_tokens value\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Check if the request was successful\n",
    "        if response.status_code != 200:\n",
    "            return jsonify({\"error\": f\"Failed to communicate with Ollama server: {response.text}\"}), response.status_code\n",
    "\n",
    "        # Extract the completion text from the Ollama server response\n",
    "        completion_response = response.json()\n",
    "        completion_text = completion_response['choices'][0]['text']\n",
    "\n",
    "        # Create the response object in the desired format\n",
    "        response = {\n",
    "            \"id\": \"chatcmpl-\" + str(int(time.time())),  # Unique ID for the completion\n",
    "            \"object\": \"chat.completion\",\n",
    "            \"created\": int(time.time()),  # Unix timestamp\n",
    "            \"model\": model,\n",
    "            \"choices\": [\n",
    "                {\n",
    "                    \"index\": 0,\n",
    "                    \"message\": {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": completion_text  # Assistant's response\n",
    "                    },\n",
    "                    \"finish_reason\": completion_response['choices'][0].get('finish_reason', 'stop')\n",
    "                }\n",
    "            ],\n",
    "            # Assuming the API provides prompt and completion token usage\n",
    "            \"usage\": {\n",
    "                \"prompt_tokens\": completion_response.get('prompt_tokens', 0),\n",
    "                \"completion_tokens\": completion_response.get('completion_tokens', len(completion_text.split())),\n",
    "                \"total_tokens\": completion_response.get('total_tokens', len(prompt.split()) + len(completion_text.split()))\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return jsonify(response)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle any errors and return a 500 status code with error message\n",
    "        return jsonify({\"error\": str(e)}), 500\n",
    "\n",
    "# Function to run the Flask app in a separate thread\n",
    "def run_app():\n",
    "    app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)\n",
    "\n",
    "# Start the Flask server in a background thread\n",
    "flask_thread = threading.Thread(target=run_app)\n",
    "flask_thread.start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
